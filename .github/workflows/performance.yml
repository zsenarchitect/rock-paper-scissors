name: ‚ö° Performance Testing & Monitoring

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    - cron: '0 3 * * 0'  # Weekly on Sunday at 3 AM
  workflow_dispatch:
    inputs:
      test-type:
        description: 'Type of performance test'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - frontend
        - backend
        - ai-training
        - memory
        - load

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'

jobs:
  # Frontend performance testing
  frontend-performance:
    name: üé® Frontend Performance
    runs-on: ubuntu-latest
    if: github.event.inputs.test-type == 'all' || github.event.inputs.test-type == 'frontend'
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
      
    - name: üì¶ Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        
    - name: üì¶ Install dependencies
      run: |
        echo "üì¶ Installing performance testing dependencies..." >> $GITHUB_STEP_SUMMARY
        cd docs
        npm init -y
        npm install --save-dev lighthouse puppeteer
        echo "‚úÖ Dependencies installed" >> $GITHUB_STEP_SUMMARY
        
    - name: üåê Start local server
      run: |
        echo "üåê Starting local server for performance testing..." >> $GITHUB_STEP_SUMMARY
        cd docs
        python3 -m http.server 8000 &
        sleep 5
        echo "‚úÖ Local server started" >> $GITHUB_STEP_SUMMARY
        
    - name: ‚ö° Run Lighthouse performance test
      run: |
        echo "‚ö° Running Lighthouse performance test..." >> $GITHUB_STEP_SUMMARY
        cd docs
        
        # Create performance log
        echo "Starting Lighthouse performance test..." > ../performance-test.log
        echo "Timestamp: $(date)" >> ../performance-test.log
        echo "URL: http://localhost:8000" >> ../performance-test.log
        echo "Node version: $(node --version)" >> ../performance-test.log
        echo "" >> ../performance-test.log
        
        # Run Lighthouse with detailed logging
        npx lighthouse http://localhost:8000 \
          --output=json \
          --output-path=lighthouse-report.json \
          --chrome-flags="--headless --no-sandbox --disable-dev-shm-usage" \
          --verbose 2>&1 | tee -a ../performance-test.log || true
          
        echo "‚úÖ Lighthouse test completed" >> $GITHUB_STEP_SUMMARY
        
    - name: üìä Analyze performance metrics
      run: |
        echo "üìä Analyzing performance metrics..." >> $GITHUB_STEP_SUMMARY
        python3 -c "
        import json
        import sys
        import traceback
        
        try:
            with open('docs/lighthouse-report.json', 'r') as f:
                report = json.load(f)
            
            # Write detailed analysis to log
            with open('../performance-analysis.log', 'w') as log:
                log.write('Lighthouse Performance Analysis\\n')
                log.write('=' * 40 + '\\n\\n')
                
                scores = report['categories']
                log.write('Performance Scores:\\n')
                for category, data in scores.items():
                    score = data['score'] * 100
                    log.write(f'  {category}: {score:.1f}/100\\n')
                    print(f'  {category}: {score:.1f}/100')
                
                # Detailed metrics
                if 'audits' in report:
                    log.write('\\nDetailed Metrics:\\n')
                    for audit, data in report['audits'].items():
                        if 'numericValue' in data:
                            log.write(f'  {audit}: {data[\"numericValue\"]} {data.get(\"unit\", \"\")}\\n')
                
                # Check if performance is acceptable
                performance_score = scores['performance']['score'] * 100
                log.write(f'\\nPerformance Threshold Check:\\n')
                log.write(f'Current Score: {performance_score:.1f}/100\\n')
                log.write(f'Threshold: 50/100\\n')
                
                if performance_score < 50:
                    log.write('‚ùå Performance score is below acceptable threshold\\n')
                    print('‚ùå Performance score too low!')
                    sys.exit(1)
                else:
                    log.write('‚úÖ Performance score is acceptable\\n')
                    print('‚úÖ Performance score acceptable')
                    
        except Exception as e:
            print(f'‚ùå Performance analysis failed: {e}')
            traceback.print_exc()
            sys.exit(1)
        "
        echo "‚úÖ Performance analysis completed" >> $GITHUB_STEP_SUMMARY
        
    - name: üìä Upload performance reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: frontend-performance-reports
        path: |
          docs/lighthouse-report.json
          performance-test.log
          performance-analysis.log
        retention-days: 30

  # Backend performance testing
  backend-performance:
    name: üêç Backend Performance
    runs-on: ubuntu-latest
    if: github.event.inputs.test-type == 'all' || github.event.inputs.test-type == 'backend'
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
      
    - name: üì¶ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: üì¶ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install psutil memory-profiler line-profiler
        
    - name: ‚ö° Run backend performance tests
      run: |
        python -c "
        import time
        import psutil
        import sys
        sys.path.append('.')
        
        # Test AI training performance
        from ai_training.models.genetic_algorithm import GeneticAlgorithm
        
        start_time = time.time()
        start_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        ga = GeneticAlgorithm()
        for i in range(100):
            ga.evolve()
            
        end_time = time.time()
        end_memory = psutil.Process().memory_info().rss / 1024 / 1024
        
        execution_time = end_time - start_time
        memory_usage = end_memory - start_memory
        
        print(f'‚è±Ô∏è Execution time: {execution_time:.2f} seconds')
        print(f'üíæ Memory usage: {memory_usage:.2f} MB')
        
        # Performance thresholds
        if execution_time > 10:
            print('‚ùå Execution time too slow!')
            sys.exit(1)
        else:
            print('‚úÖ Execution time acceptable')
            
        if memory_usage > 100:
            print('‚ùå Memory usage too high!')
            sys.exit(1)
        else:
            print('‚úÖ Memory usage acceptable')
        "
        
    - name: üìä Memory profiling
      run: |
        python -c "
        from memory_profiler import profile
        import sys
        sys.path.append('.')
        
        @profile
        def test_memory():
            from ai_training.models.neural_network import NeuralNetwork
            nn = NeuralNetwork()
            for i in range(50):
                nn.forward([0.1, 0.2, 0.3])
            return nn
            
        test_memory()
        " > memory-profile.txt || true
        
    - name: üìä Upload performance data
      uses: actions/upload-artifact@v3
      with:
        name: backend-performance
        path: memory-profile.txt
        retention-days: 30

  # AI Training performance
  ai-performance:
    name: ü§ñ AI Training Performance
    runs-on: ubuntu-latest
    if: github.event.inputs.test-type == 'all' || github.event.inputs.test-type == 'ai-training'
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
      
    - name: üì¶ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: üì¶ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: ‚ö° Test AI training performance
      run: |
        python ai_training/scripts/train_model.py --performance-test --iterations=100
        
    - name: üìä Benchmark AI algorithms
      run: |
        python -c "
        import time
        import sys
        sys.path.append('.')
        
        # Benchmark genetic algorithm
        from ai_training.models.genetic_algorithm import GeneticAlgorithm
        
        print('üß¨ Genetic Algorithm Benchmark:')
        start = time.time()
        ga = GeneticAlgorithm()
        for i in range(1000):
            ga.evolve()
        ga_time = time.time() - start
        print(f'  1000 iterations: {ga_time:.2f}s')
        
        # Benchmark neural network
        from ai_training.models.neural_network import NeuralNetwork
        
        print('üß† Neural Network Benchmark:')
        start = time.time()
        nn = NeuralNetwork()
        for i in range(1000):
            nn.forward([0.1, 0.2, 0.3])
        nn_time = time.time() - start
        print(f'  1000 forward passes: {nn_time:.2f}s')
        
        # Performance thresholds
        if ga_time > 5:
            print('‚ùå GA too slow!')
            sys.exit(1)
        if nn_time > 1:
            print('‚ùå NN too slow!')
            sys.exit(1)
            
        print('‚úÖ AI performance acceptable')
        "

  # Memory usage testing
  memory-test:
    name: üíæ Memory Testing
    runs-on: ubuntu-latest
    if: github.event.inputs.test-type == 'all' || github.event.inputs.test-type == 'memory'
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
      
    - name: üì¶ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: üì¶ Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install memory-profiler psutil
        
    - name: üíæ Test memory usage patterns
      run: |
        python -c "
        import psutil
        import gc
        import sys
        sys.path.append('.')
        
        def test_memory_leaks():
            from ai_training.models.genetic_algorithm import GeneticAlgorithm
            from ai_training.models.neural_network import NeuralNetwork
            
            initial_memory = psutil.Process().memory_info().rss / 1024 / 1024
            print(f'Initial memory: {initial_memory:.2f} MB')
            
            # Create and destroy objects multiple times
            for cycle in range(10):
                ga = GeneticAlgorithm()
                nn = NeuralNetwork()
                
                # Use objects
                for i in range(100):
                    ga.evolve()
                    nn.forward([0.1, 0.2, 0.3])
                
                # Delete objects
                del ga, nn
                gc.collect()
                
                current_memory = psutil.Process().memory_info().rss / 1024 / 1024
                print(f'Cycle {cycle + 1} memory: {current_memory:.2f} MB')
            
            final_memory = psutil.Process().memory_info().rss / 1024 / 1024
            memory_increase = final_memory - initial_memory
            
            print(f'Final memory: {final_memory:.2f} MB')
            print(f'Memory increase: {memory_increase:.2f} MB')
            
            if memory_increase > 50:
                print('‚ùå Potential memory leak detected!')
                sys.exit(1)
            else:
                print('‚úÖ No memory leaks detected')
        
        test_memory_leaks()
        "

  # Load testing
  load-test:
    name: üî• Load Testing
    runs-on: ubuntu-latest
    if: github.event.inputs.test-type == 'all' || github.event.inputs.test-type == 'load'
    
    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v4
      
    - name: üì¶ Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: üì¶ Install load testing tools
      run: |
        python -m pip install --upgrade pip
        pip install locust requests
        
    - name: üî• Run load tests
      run: |
        # Start the application
        cd docs
        python3 -m http.server 8000 &
        sleep 5
        
        # Run load test
        python -c "
        import requests
        import time
        import threading
        from concurrent.futures import ThreadPoolExecutor
        
        def make_request():
            try:
                response = requests.get('http://localhost:8000', timeout=5)
                return response.status_code == 200
            except:
                return False
        
        # Test concurrent requests
        print('üî• Load Testing:')
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=10) as executor:
            futures = [executor.submit(make_request) for _ in range(100)]
            results = [f.result() for f in futures]
        
        end_time = time.time()
        success_rate = sum(results) / len(results) * 100
        total_time = end_time - start_time
        
        print(f'  Requests: 100')
        print(f'  Success rate: {success_rate:.1f}%')
        print(f'  Total time: {total_time:.2f}s')
        print(f'  RPS: {100/total_time:.1f}')
        
        if success_rate < 95:
            print('‚ùå Load test failed!')
            exit(1)
        else:
            print('‚úÖ Load test passed!')
        "

  # Performance regression detection
  performance-regression:
    name: üìà Performance Regression
    runs-on: ubuntu-latest
    needs: [frontend-performance, backend-performance, ai-performance, memory-test, load-test]
    if: always()
    
    steps:
    - name: üì• Download performance artifacts
      uses: actions/download-artifact@v3
      
    - name: üìà Analyze performance trends
      run: |
        echo "# ‚ö° Performance Analysis" > performance-analysis.md
        echo "" >> performance-analysis.md
        echo "## Test Results" >> performance-analysis.md
        echo "- Frontend Performance: ${{ needs.frontend-performance.result }}" >> performance-analysis.md
        echo "- Backend Performance: ${{ needs.backend-performance.result }}" >> performance-analysis.md
        echo "- AI Performance: ${{ needs.ai-performance.result }}" >> performance-analysis.md
        echo "- Memory Test: ${{ needs.memory-test.result }}" >> performance-analysis.md
        echo "- Load Test: ${{ needs.load-test.result }}" >> performance-analysis.md
        echo "" >> performance-analysis.md
        echo "## Recommendations" >> performance-analysis.md
        echo "1. Monitor performance metrics over time" >> performance-analysis.md
        echo "2. Set up performance alerts" >> performance-analysis.md
        echo "3. Optimize slow components" >> performance-analysis.md
        echo "4. Consider caching strategies" >> performance-analysis.md
        
    - name: üìä Upload performance analysis
      uses: actions/upload-artifact@v3
      with:
        name: performance-analysis
        path: performance-analysis.md
        retention-days: 30
